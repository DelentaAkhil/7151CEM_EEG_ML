{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCe6I7T2Xq6a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_excel(r'/content/drive/MyDrive/EEG_Dataset_Decoded/Training_data.xlsx',header=None)"
      ],
      "metadata": {
        "id": "k2gXjEqBcIRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.shape)\n",
        "print(\"Number of rows: \"+str(train_data.shape[0]))\n",
        "print(\"Number of columns: \"+str(train_data.shape[1]))"
      ],
      "metadata": {
        "id": "yJoyuwtIcPYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = pd.read_excel(r'/content/drive/MyDrive/EEG_Dataset_Decoded/Training_labels.xlsx',header=None)"
      ],
      "metadata": {
        "id": "aLy7zNgtbzgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_labels.shape)\n",
        "print(\"Number of rows: \"+str(train_labels.shape[0]))\n",
        "print(\"Number of columns: \"+str(train_labels.shape[1]))"
      ],
      "metadata": {
        "id": "3T-wWLyBb7Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "KWKSltfm7Da0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.value_counts() # To find the dataset is balanced or not: In this case it is balanced"
      ],
      "metadata": {
        "id": "6xdG5accbwSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.isnull().values.any() # Machine Learning models will not accept null values. In this dataset there are no null values."
      ],
      "metadata": {
        "id": "nCtgpibEdw5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "pxdVPup6eY5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "ymvrXQhU7XBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define min max scaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler_train = MinMaxScaler()\n",
        "# transform data\n",
        "scaled_train_data = scaler_train.fit_transform(train_data)\n",
        "print(scaled_train_data)"
      ],
      "metadata": {
        "id": "9J2n7_iqkpt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define min max scaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler_test = MinMaxScaler()\n",
        "# transform data\n",
        "scaled_test_data = scaler_train.fit_transform(test_data)\n",
        "print(scaled_test_data)"
      ],
      "metadata": {
        "id": "drKT4qh5lpqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_train_data"
      ],
      "metadata": {
        "id": "ksIhcs43lIh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dimensionality Reduction"
      ],
      "metadata": {
        "id": "zMsbf0fM81BP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA\n",
        "from sklearn.decomposition import PCA\n",
        "import time"
      ],
      "metadata": {
        "id": "X6Z5KORXhZwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Data Dimensionality reduction\n",
        "%%time\n",
        "start = time.time()\n",
        "# Program to find the optimal number of components for PCA\n",
        "n_comp1 = [2,3,4,10,15,20,25, 40, 50, 75, 100,150,200,500] # list containing different values of components\n",
        "explained1 = [] # explained variance ratio for each component of PCA\n",
        "for x1 in n_comp1:\n",
        "    pca = PCA(n_components=x1)\n",
        "    pca.fit(scaled_train_data)\n",
        "    explained1.append(pca.explained_variance_ratio_.sum())\n",
        "    print(\"Number of components = %r and explained variance = %r\"%(x1,pca.explained_variance_ratio_.sum()))\n",
        "plt.plot(n_comp1, explained1)\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel(\"Explained Variance\")\n",
        "plt.title(\"Plot of Number of components v/s explained variance\")\n",
        "plt.show()\n",
        "print('Duration: {} seconds'.format(time.time() - start))"
      ],
      "metadata": {
        "id": "TBqISyf_e1xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "pca = PCA(n_components=25)\n",
        "principalComponents_train = pca.fit_transform(scaled_train_data)\n",
        "print('Duration: {} seconds'.format(time.time() - start))\n",
        "train_pca = pd.DataFrame(data = principalComponents_train)\n",
        "#              , columns = ['principal component 1', 'principal component 2','principal component 3'])"
      ],
      "metadata": {
        "id": "LmWyKqM9hTPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pca"
      ],
      "metadata": {
        "id": "3LtuZR-NkY99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "pca = PCA(n_components=25)\n",
        "principalComponents_test = pca.fit_transform(scaled_test_data)\n",
        "print('Duration: {} seconds'.format(time.time() - start))\n",
        "test_pca = pd.DataFrame(data = principalComponents_test)\n",
        "#              , columns = ['principal component 1', 'principal component 2','principal component 3'])"
      ],
      "metadata": {
        "id": "j0b36_9vmEet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pca"
      ],
      "metadata": {
        "id": "4rMr7jDnkd9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_out = train_labels[0].ravel()"
      ],
      "metadata": {
        "id": "v4tlkMF4nAu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.info()"
      ],
      "metadata": {
        "id": "LgTG1Y22tDUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_out"
      ],
      "metadata": {
        "id": "IohLbLSntlfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test Split for ML Model Creation"
      ],
      "metadata": {
        "id": "Q3SQUxG_-Sef"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSdVjFzW-R9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_Train, X_Test, Y_Train, Y_Test = train_test_split(train_pca, train_out, test_size = 0.20, random_state = 100)"
      ],
      "metadata": {
        "id": "IqvnYbxUurDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier # Non-Linear Classifier\n",
        "from sklearn import svm # Linear Classifier\n",
        "classifier = RandomForestClassifier() # Classifier is instantiated\n",
        "classifier.fit(X_Train,Y_Train) # Model Training"
      ],
      "metadata": {
        "id": "73HOdHx_nJV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_Pred = classifier.predict(X_Test) # Test Testing"
      ],
      "metadata": {
        "id": "-e82lMHmpu6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn import metrics \n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "matrix = confusion_matrix(Y_Test, Y_Pred) # Y-test = True Value,Y_Pred = Predicting Values \n",
        "sns.heatmap(matrix, annot=True, fmt=\"d\")\n",
        "plt.title('Confusion Matrix_RF')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "print(classification_report(Y_Test, Y_Pred))"
      ],
      "metadata": {
        "id": "kxc3GAqkqMoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "classifier_svc = svm.SVC()\n",
        "classifier_svc.fit(X_Train,Y_Train)"
      ],
      "metadata": {
        "id": "W62nTQdJ0JVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_Pred_svc = classifier_svc.predict(X_Test)"
      ],
      "metadata": {
        "id": "I2IkybpA0RfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn import metrics \n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "matrix = confusion_matrix(Y_Test, Y_Pred_svc)\n",
        "sns.heatmap(matrix, annot=True, fmt=\"d\")\n",
        "plt.title('Confusion Matrix_SVC')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "print(classification_report(Y_Test, Y_Pred_svc))"
      ],
      "metadata": {
        "id": "IuI5OxCJ0Vo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dyF5U5VtItun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import XGBClassifier\n",
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "YFeeAbQ3qjF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# declare parameters\n",
        "params = {\n",
        "            'objective':'multi:softprob',\n",
        "            'max_depth': 50,\n",
        "            'alpha': 10,\n",
        "            'learning_rate': 0.1,\n",
        "            'n_estimators':100\n",
        "        }\n",
        "            \n",
        "            \n",
        "            \n",
        "# instantiate the classifier \n",
        "xgb_clf = XGBClassifier(**params)\n",
        "# xgb_clf = XGBClassifier()"
      ],
      "metadata": {
        "id": "RlKzbxtDxhQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_clf.fit(X_Train,Y_Train)"
      ],
      "metadata": {
        "id": "UjawKz1HyEqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_Pred_xgb = xgb_clf.predict(X_Test)"
      ],
      "metadata": {
        "id": "_qzwuARfyL_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn import metrics \n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "matrix = confusion_matrix(Y_Test, Y_Pred_xgb)\n",
        "sns.heatmap(matrix, annot=True, fmt=\"d\")\n",
        "plt.title('Confusion Matrix_XGB')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "print(classification_report(Y_Test, Y_Pred_xgb))"
      ],
      "metadata": {
        "id": "iUMdPA4FyV4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k7k8vkwIyjKk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}